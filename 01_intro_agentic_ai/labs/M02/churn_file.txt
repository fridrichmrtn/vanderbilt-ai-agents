User Churn Model in E-CommerceRetailScientific Papers of the Universityof Pardubice, Series D: Faculty ofEconomics and Administration2022, 30(1), 1478.©The Author(s) 2022. This isan open access article underthe CC-BY 4.0 license.DOI: 10.46585/sp30011478editorial.upce.cz/SciPapMartin FridrichBrno University of Technology, Faculty of Business and Management, Institute of Informatics, Czech RepublicPetr DostálBrno University of Technology, Faculty of Business and Management, Institute of Informatics, Czech RepublicAbstractIn e-commerce retail, maintaining a healthy customer base through retention management is necessary. Churnprediction efforts support the goal of retention and rely upon dependent and independent characteristics. Unfortunately,there does not appear to be a consensus regarding a user churn model. Thus, our goal is to propose a model based ona traditional and new set of attributes and explore its properties using auxiliary evaluation. Individual variable importanceis assessed using the best performing modeling pipelines and a permutation procedure. In addition, we estimate theeffects on the performance and quality of a feature set using an original technique based on importance ranking andinformation retrieval. The performance benchmark reveals satisfying pipelines utilizing LR, SVM-RBF, and GBMlearners. The solutions rely profoundly on traditional recency and frequency aspects of user behavior. Interestingly,SVM-RBF and GBM exploit the potential of more subtle elements describing user preferences or date-time behaviouralpatterns. The collected evidence may also aid business decision-making associated with churn prediction efforts, e.g.,retention campaign design.KeywordsUser Model, Churn Prediction, Customer Relationship Management, Electronic Commerce, Retail, Machine Learning,Feature Importance, Feature Set ImportanceJEL ClassificationC60, M31IntroductionDuring the last two decades, the unprecedented growth of the retail e-commerce industry has created a competitiveenvironment built upon immense technological advances and a user-centric paradigm (MacKenzie et al., 2013;Morgan, 2018; Terdiman, 2018). It has become essential for organizations to sustain a valuable user/customerbase. Churn mitigation is principally motivated by a disparity among the unitary costs of user acquisition andretention (Gronwald, 2017); however, there are additional benefits (Ascarza et al., 2018).The retention strategy is arranged into short- and long-term pursuits (Mezghani et al., 2012; Ascarza et al., 2018).In the short term, companies aim to capture behavioural and transactional dynamics to identify users at imminentrisk of churning and preserve them if profitable. In the long term, firms lean towards enhancing customer satisfactionand loyalty through understanding the drivers of churn. Nevertheless, the success of both pursuits is highlydependent on appropriate representation. Brusilovsky (1996) proposes a user model concept to describe userfeatures such as behavior, goals, knowledge, stereotypes, and preferences linked to practical actions. Its qualitycan be assessed indirectly with machine learning methods. Unfortunately, there is no clear consensus on auser/customer churn model concerning both explanatory and explained characteristics.Hence, we aim to propose a user churn model for e-commerce retail and explore its properties using indirectevaluation. The rest of the article is structured as follows: the research literature concerning the user model,modeling, and evaluation is covered in the second section. The user churn model is introduced and reviewed inthe third section. The following part defines the dataset, approach to predictive modeling, and performanceevaluation. Furthermore, the fourth section details feature and feature set assessment methods. The fifth sectionoutlines the collected outcomes critically examined in the last part of the paper.
Page 2
2 SciPap 30(1)Literature ReviewTo outline the relevant research endeavours, we concentrate on peer-reviewed articles from the Web of Scienceand Scopus databases, dealing with user/customer churn in the e-commerce environment (see Table 1). Thestudies determine customer churn in non-contractual settings analytically, and neither probability models (Fader etal., 2005; Netzer et al., 2008) nor cost-benefit frameworks (Glady et al., 2009; Clemente-Císcar et al., 2014) areemployed. Gordini and Veglio (2017), Li and Li (2019), Chou and Chuang (2018), and Llave Montinel and López(2020) use the subsequent periods without financial transactions to ascertain customer churn events. Rachid et al.(2018) extend this notion by combining defection levels with changes in transactional behavior. However, Abbasiet al. (2015), Lee et al. (2018), Rothmeier et al. (2020), and Berger and Kompan (2019) recognize the churn eventin the e-commerce domain as dropping out of the buying process or defecting across web or game sessions.Gordini and Veglio (2017) and Li and Li (2019) deem transactional features (e.g., last transaction date, number offinancial transactions, total revenue) the most relevant and completely omit the domain-specific ones. By contrast,Rachid et al. (2018), Berger and Kompan (2019), Rothmeier et al. (2020), and Perisic and Pahor (2021) show thatthe combination of transactional and behavior/usage (e.g., number of sessions, session length, conversion rate)can significantly improve the predictive performance of the model. Other appealing characteristics, such asperceptional features (Almuqren et al., 2021), decision stages (Abbasi et al., 2015), or geospatial patterns (LlaveMontinel and López, 2020), are included. However, the evidence of their importance may be anecdotal.The building blocks of the modeling pipeline involve data processing, sampling, dimensionality reduction, ormodeling and evaluation, depending on the researcher's objectives. Nonetheless, most articles concentrate on thelast step, favouring a train–test split experimental design and confusion matrix-based performance metrics (Yu etal., 2011; Kim and Lee, 2012; Abbasi et al., 2015; Llave Montinel and López, 2020, Chou and Chuang, 2018;Almuqren et al., 2021). The prevalent classification algorithms include logistic regression (LR), support vectormachine (SVM), artificial neural networks (ANN), and meta-learners.Table 1. Selected literature on user/customer churn prediction in e-commerce.Research Article Experiment Modeling Performance MetricsAbbasi et al. (2015) train–test splits SVM, Bayesian nets ACC, PRE, REC, F1Almuqren et al. (2021) train–test splits ANN PRE, REC, F1Berger and Kompan (2019) cross-validation Birch, K-means, SVM ACC, PRE, REC, AUCChou and Chuang (2018) train–test splits GAM, Bagging, Boosting AUCGordini and Veglio (2016) cross-validation, train–testsplits LR, SVM, ANN ACC, AUC, LIFTKim and Lee (2012) train–test splits LR, ANN, Bagging REC, AUC, LIFTLi and Li (2019) train–validation–test splits LR, Gradient boosting ACC, PRE, RECLlave Montinel and López(2020) train–test splits GAM, Spatial probit 1-ACC, AUCPerisic and Pahor (2021) cross-validation LR, Bagging AUCRachid et al. (2018) cross-validation Decision tree, ANN, Bagging ACC, PRE, REC, F1Rothmeier et al. (2020) cross-validation LR, Decision tree, SVM, ANN,Bagging, Boosting, Others AUCYu et al. (2011) train–test splits Decision tree, SVM, ANN ACC, PRE, REC, LIFTSource: AuthorsUser Churn ModelTo propose a viable user churn model, we adopt a practical suggestion by Tamaddoni Jahromi et al. (2014) anduse the accessible, fundamental attributes while maximizing the model's predictive power. We form the modelaround user-item interactions, data generally available to any e-commerce retail store.The churn event (dependent variable) is characterized as interaction/no interaction with the e-commerce websiteduring the subsequent month; thus, the explained variable is binary. This interpretation allows us to reflect on onlinebehavior over a considerable period. Other works define churn ordinarily as transactional (Gordini and Veglio,2017; Li and Li, 2019) or employing real-time interactions (Abbasi et al., 2015; Berger and Kompan, 2019).
Page 3
3 SciPap 30(1)The user model (independent variables) consists of six sets of attributes: recency, frequency, monetary, categoryand item, date–time, and other characteristics (see Table 2). The first three sets cover established behaviouralperspectives. Hughes (2012) describes them as Recency—the time since the last transaction, where a short spanindicates a high probability of returning customer; Frequency—the number of transactions within a period, wherehigh frequency corresponds to increased loyalty; and Monetary—the total revenue within a period, where highspending marks high valued customers. We extend the outlined notions to the online environment with session-and interaction-level features. The scarcity of purchasing data also drives this shift. Matching dimensions arepresent across all reviewed articles. Gordini and Veglio (2017) and Li and Li (2019) acknowledge that transactionalcharacteristics are the most influential. On the other hand, Berger and Kompan (2019) and Rachid et al. (2018)demonstrate that the amalgam of transactional and e-commerce behavior/usage leads to notable gains inclassification performance.Category & item set aims to capture a user's preference. Interest in a vast range of products may be a sign of aloyal customer (Mozer et al., 2000); issues in a particular category, on the other hand, may lead to customerdefection (Buckinx and Dirk, 2005). While research endeavors support categorical behavior (Gordini and Veglio,2017), its impact on user/customer churn in the online context seems understudied.Date & time attributes reflect the prospect of a different user experience level during the day (Buckinx and Dirk,2005), e.g., peaking URL requests during noon may lead to an inadequate response time and worsen theexperience. Berger and Kompan (2019) consider comparable attributes for real-time sessions; however, theauthors do not evaluate the variable's importance directly.Others include time-to-event features and average session length. We expect a long session or a short timebetween interactions to indicate a more engaged user. Again, Berger and Kompan (2019) introduce a similarperspective.Table 2. User Churn model.Set Attribute Description Variable Data TypeRecencysession recency time difference from the last user sessionand current (split) date [days] ses_rec floataverage session recency average period between sessions [days] ses_rec_avg floatstandard deviation in sessionrecencystandard deviation in time between sessions[days] ses_rec_sd floatcv session recencyratio of standard deviation in time to sessionto average time to session (coefficient ofvariation) [%]ses_rec_cv floatuser maturity difference between the start of the first usersession and current (split) date [days] user_rec floatFrequencysession frequency session count [n] ses_n intrelative session frequency ratio of session frequency to accountmaturity [session/a day] ses_n_r floatuser–app interactionfrequencyuser–application interaction (view/add-to-cart/buy clicks) count [n] int_n intrelative user–app interactionfrequencyratio of user–app interaction frequency tosession frequency [int/session] int_n_r floattransactional frequency transaction count [n] tran_n intrelative transactionalfrequencyratio of transactional frequency to sessionfrequency (individual conversionrate) [transaction/session]tran_n_r floatMonetarytransactional revenue total revenue [USD] rev_sum floatrelative transactional revenue total revenue to session frequency[USD/session] rev_sum_r floatabove average transactionalrevenueproportion of sessions with above-averagespending [%] major_spend_r floatCategory& iteminteractions across root-levelcategoriessum of interactions across root-levelcategories [n]int_cat1_n:int_cat24_n intrelative user-cat interactionfrequencyaverage no of distinct root-level categoriesinteracted in session [n] int_cat_n_avg floatrelative user-item interactionfrequencyaverage no of distinct items interacted insession [n] int_itm_n_avg float
Page 4
4 SciPap 30(1)Date& timeaverage month average month (session start) ses_mo_avg floatstandard deviation in months standard deviation in months ses_mo_sd floataverage day-hour average hour of a day (session start) ses_hr_avg floatstandard deviation in day-hours standard deviation in hours of a day ses_hr_sd floatweekend proportion weekend sessions proportion [%] ses_wknd_r floatOthersaverage session length average session duration [min] ses_len_avg floattime to interaction average time between interactions within asession [mins] time_to_int floattime to transaction average time betweentransactional events [days] time_to_tran floatSource: AuthorsResearch Methodology and ImplementationIn this section, we describe the components supporting the indirect assessment of the user churn model, namely (1)the dataset and its properties, (2) building blocks of modeling pipelines, and (3) feature importance estimationprocedures. Furthermore, we accompany the work with code and data repositories to ensure transparency andreproducibility; see the Supplementary section.Fig. 1. Lower triangular matrix of Pearson's correlation coefficients within the user churn model.Source: AuthorsDatasetWe introduce an open e-commerce retail dataset based on the Retail Rocket Dataset. It covers the period of2015/05/09–2015/09/17 and consists of 49,358 observations, 47 user model vectors, and the churn event. Thetarget class distribution is imbalanced, with a churn rate of ~89%.Further univariate exploratory analysis reveals that most attributes are asymmetric and suffer from outliers;Monetary and Category & item characteristics exhibit sparsity. The multivariate investigation revealsmulticollinearity inside the user churn model; see the lower triangular matrix of Pearson's correlation coefficients in
Page 5
5 SciPap 30(1)Figure 1. The elements are sorted using agglomerative clustering with Ward linkage, which allows us to seeunderlying groups of associated features, e.g., the light triangle in the upper-left corner represents a coherentcluster of positively correlated features, which establishes a weak negative association with the lower-right set offeatures, including the target class. See the Supplementary Materials section for further details.Modeling PipelineData ProcessingThe transformation steps ordinarily ascertain that the basic assumptions of the downstream techniques aresatisfied. Following the exploratory analysis, we concentrate on the explanatory variables. We impute missingvalues, omit the vectors with near-zero variance, and add a second-degree polynomial expansion to reduce thebias. Furthermore, we approach the disparity in unit measurements and irregularity in the shape of the observedprobability distributions with a uniform quantile transformation.Feature ExtractionTo mitigate multicollinearity and sparsity amongst the explanatory variables, we apply the principal componentanalysis technique to project the processed data into orthonormal 50-dimensional space while capturing the mostvariability. Additional benefits may include improvements in the predictive performance and generalization andreductions in computational runtime and memory requirements (Aggarwal, 2014).ModelingThere is a substantial amount of research devoted to the algorithm selection problem. Nevertheless, we decidedto employ classification methods prevalent in the user/customer churn domain (see Table 1). For the most part, weuse out-of-the-box implementations and hyperparameter settings hinged on the scikit-learn library (Pedregosa etal., 2011), with SVMs being an exception (see Table 3).Table 3. Classification algorithms and implementation notes.Family Algorithm Implementation Notes Abbreviationgeneralized linear models regularized logisticregressionL2 penalty LRsupport vector machines support vector machine witha linear kernelL2 penalty, Platt's method for probabilityestimatesSVM-LINsupport vector machines support vector machine witha radial basis kernelexplicit mapping with Nystroem kernel, L2penalty, Platt's method for probabilityestimatesSVM-RBFartificial neural networks multi-layer perceptron 1 hidden layer with 100 RELU units, stochasticgradient descent, 200 epochs, L2 penaltyMLPmeta-learning (bagging) random forest 100 decision trees, Gini criterion RFmeta-learning (boosting) gradient boosting machine 100 decision trees, Friedman MSE criterion GBMSource: AuthorsPerformance BenchmarkExperimental DesignTo secure a reliable performance benchmark, we utilize a stratified 20-fold cross-validation scheme (see Figure 2).Firstly, we obtain training and validation data partitions. The modeling pipeline is instantiated and fitted using thetraining split, and its performance is assessed on both data partitions. The procedure is repeated until each splitacts as a validation set; intermediate pipeline and outcomes are collected and stored.
Page 6
6 SciPap 30(1)Fig. 2. Performance benchmark experimental design.Source: AuthorsClassification MetricsWe assess the classification capabilities with well-known confusion-matrix-based and subjective independentmeasures. We select Accuracy for its comprehensibility and reflect on the distribution imbalance in the target classwith the F1 score (harmonic mean of the Precision & Recall). Furthermore, we select the threshold-independentArea under the receiver operating characteristic curve. This metric describes the probability that randomly drawnmembers of the retained class will produce a lower churn score than randomly drawn churners. More specifically,let us have a class membership score 𝑠 = 𝑠(𝑥), as a function of user model vector 𝑥; the probability density functionof corresponding scores 𝑓𝑘(𝑠), with a cumulative distribution 𝐹𝑘(𝑠), and classes 𝑘 𝜖 {0,1}; then,𝐴𝑈𝐶 = ∫ 𝐹0(𝑠)𝑓1(𝑠)𝑑𝑠.∞−∞(1)Thus, it reflects on the learner's ranking capacity. However, AUC also suffers from a few conceptualshortcomings (Hand, 2009).Feature ImportanceThis article strives to assess individual feature importance and feature set importance. To address the former, weuse the permutation technique Breiman (2001) proposed. It measures the variation in the performance of themodeling solution when a feature is randomly reordered. We construct the empirical probability distribution for eachexplanatory variable using twenty validation splits and ten repeated permutations.We propose an original approach based on the information retrieval theory to address the latter. We treat eachfeature set as retrieved documents and the most influential features identified in the previous step as relevantdocuments, which allows us to evaluate the set importance using the following. Let us have a set of features G anda set of the n most influential features 𝐹𝑛; then,𝑃𝑅𝐸𝐺 = |𝐺 ∩ 𝐹𝑛||𝐺| , (2)𝑅𝐸𝐶𝐺 = |𝐺 ∩ 𝐹𝑛|𝑛 , (3)𝐹𝛽𝐺 = (1 + 𝛽2) ∙ 𝑃𝑅𝐸𝐺 ∙ 𝑅𝐸𝐶𝐺(𝛽2 ∙ 𝑃𝑅𝐸𝐺 ) + 𝑅𝐸𝐶𝐺, (4)where 𝑃𝑅𝐸𝐺 indicates the proportion of the relevant features within the set; 𝑅𝐸𝐶𝐺 shows the ratio of the influentialelements captured; 𝐹𝛽𝐺 allows us to combine both perspectives with the weighted harmonic mean. We considerthe ten most influential features and apply a non-preferential variant of 𝐹𝛽𝐺 , with 𝛽 = 1. As a result, we can composeempirical probability distributions for the information retrieval measures. In addition, we introduce their referentialcounterparts based on 10,000 random permutations in the feature importance rank.ResultsThis section centres around (1) classification performance and runtimes and (2) individual and set feature
Page 7
7 SciPap 30(1)importance. We analyse the modeling pipelines and choose the most suitable ones for the feature permutationassessment, which is the article's main focus. The analysis is supported by estimates of central tendencies,confidence bounds, and hypothesis testing with Bonferroni corrections on untreated 𝛼 = 0.01.Performance BenchmarkWe compare the prediction ability of the modeling pipelines using the mean point estimates of the classificationmetrics and confidence bounds for the underlying distributions in Table 4. In addition, we evaluate the differencein performance for each combination of pipelines using a paired t-test coupled with validation splits. The nullhypothesis claims that the true difference in sample means equals zero; the alternative hypothesis states that theactual difference in sample means is not equal to zero, i.e., there is a statistically significant difference inclassification performance between the two solutions.Table 4. Classification performance metrics computed over the validation data partitions.Algorithm Training time [s] Prediction time [s] ACC (95% CI) F1 (95% CI) AUC (95% CI)LR 6.23 (±0.52) 0.09 (±0.01) 0.8894 (±0.0010) 0.9410 (±0.0005) 0.7374 (±0.0082)SVM-LIN 109.19 (±8.19) 0.08 (±0.01) 0.8888 (±0.0010) 0.9406 (±0.0005) 0.7315 (±0.0076)SVM-RBF 14.65 (±1.22) 0.12 (±0.01) 0.8879 (±0.0011) 0.9401 (±0.0006) 0.7345 (±0.0079)MLP 46.62 (±3.64) 0.10 (±0.02) 0.8762 (±0.0018) 0.9327 (±0.0010) 0.6869 (±0.0086)RF 68.19 (±3.7) 0.22 (±0.02) 0.8865 (±0.0010) 0.9391 (±0.0005) 0.7102 (±0.0085)GBM 103.87 (±6.01) 0.08 (±0.01) 0.8894 (±0.0010) 0.9409 (±0.0005) 0.7426 (±0.0081)Source: AuthorsThe LR, SVM, and GBM algorithms are almost on par in the threshold-dependent confidence matrix-based ACCand F1, with LR being the best-performing classifier. We fail to reject the null hypothesis for the pairs and metrics,i.e., there does not seem to be enough evidence to distinguish between the solutions. The outcomes of subject-independent AUC are more diverse, with GBM achieving the highest value. We still fail to reject the null hypothesisfor associations amongst LR, SVM-RBF, and GBM. The remaining solutions do not perform well; the gap isstatistically significant and aligned with acceptance of the alternative hypothesis in most (RF) or all pairedcomparisons (MLP).Fig. 3. Bias and variance trade-off in AUC.Source: Authors
Page 8
8 SciPap 30(1)To understand the underwhelming performance of RF and MLP, we present the bias-variance trade-off plot inFigure 3, concentrating on AUC on both seen and unseen data. We recognize that both algorithms exhibitoutstanding classification ability on training splits (low bias); however, they fail to generalize on validation splits(high variance). In other words, their out-of-the-box hyperparameter settings lead to unreasonably complex models.Nonetheless, hyperparameter optimization is beyond the scope of this article.The disparities in computational runtime reveal additional practical insights, i.e., the training LR pipeline is fasterthan GBM by a factor of ~16 while yielding equivalent classification performance. In addition, we see that theNystroem kernel mapping (Williams and Seeger, 2000) improves the training phase of the SVM solution by a factorof ~7. Thus, LR and SVM-RBF may be favoured due to their parsimony and speed.Feature ImportanceOverall, permutation feature importance is evaluated on validation data partitions using the three best-performingsolutions: LR, SVM-RBF, and GBM. The most influential variables are presented in Table 5, with respective meanpoint estimates and confidence bounds. We also verify that the decrease in AUC is positive for all of them with thebootstrap test. Variables ses_rec, ses_rec_avg, and ses_n are indispensable for all three pipelines and are alignedwith our expectations, e.g., we would suspect a user with a recent session, a low average period between sessions,and a high number of sessions to be retained.Table 5. Individual permutation feature importance.LR SVM-RBF GBMVariable Decrease inAUC (95% CI) Variable Decrease inAUC (95% CI) Variable Decrease inAUC (95% CI)ses_rec 0.0731 (±0.0019) ses_rec 0.0791 (±0.0025) ses_rec 0.0617 (±0.0015)ses_rec_avg 0.0256 (±0.0012) ses_rec_avg 0.0150 (±0.0009) ses_n 0.0157 (±0.0009)ses_n 0.0156 (±0.0009) ses_n 0.0118 (±0.0008) ses_rec_avg 0.0151 (±0.0009)int_n 0.0097 (±0.0007) int_n 0.0088 (±0.0008) ses_mo_sd 0.0104 (±0.0008)user_rec 0.0070 (±0.0006) ses_mo_sd 0.0070 (±0.0009) ses_rec_cv 0.0093 (±0.0008)ses_rec_cv 0.0069 (±0.0006) int_cat17_n 0.0059 (±0.0008) ses_rec_sd 0.0065 (±0.0007)ses_n_r 0.0064 (±0.001) int_cat22_n 0.0057 (±0.0006) ses_n_r 0.0060 (±0.0006)ses_rec_sd 0.0021 (±0.0004) user_rec 0.0044 (±0.0005) ses_mo_avg 0.0056 (±0.0007)int_cat17_n 0.0021 (±0.0004) ses_rec_cv 0.0042 (±0.0007) int_n 0.0054 (±0.0006)ses_mo_avg 0.0014 (±0.0002) int_cat20_n 0.0041 (±0.0006) user_rec 0.0044 (±0.0006)Source: AuthorsFeature set importance examines intersections between influential feature sets constructed in the previous stepand each feature group employing information retrieval measures. The results are shown in Table 6, withcorresponding mean estimates and confidence intervals supported by the bootstrap tests. The Recency set exhibitsoutstanding qualities with a very high F1; 65–86 % of its elements are identified as relevant (PRE); modelingsolutions select 33–43 % of the essential characteristics from the Recency set (REC). The Frequency group is therunner-up, with a considerable F1; 36–52% PRE and 25–32% REC in LR and GBM. Category & item and Date &time characteristics display a moderate F1; the former suffers from sparsity (low PRE) and is the essential set forSVM-RBF (high REC); the latter group shows acceptable PRE and is favoured by GBM (fair REC). The remainingfeature sets appear irrelevant for all modeling pipelines; however, we can statistically confirm this only for LR.Table 6. Feature set importance.Set Algorithm PRE (95% CI) REC (95% CI) F1 (95% CI)RecencyLR 0.863 (±0.017) 0.432 (±0.008) 0.576 (±0.011)SVM-RBF 0.651 (±0.022) 0.325 (±0.011) 0.434 (±0.015)GBM 0.805 (±0.022) 0.402 (±0.011) 0.537 (±0.015)Frequency LR 0.524 (±0.016) 0.315 (±0.010) 0.393 (±0.012)
Page 9
9 SciPap 30(1)SVM-RBF 0.362 (±0.021) 0.217 (±0.013) 0.272 (±0.016)GBM 0.414 (±0.016) 0.248 (±0.010) 0.310 (±0.012)MonetaryLR 0.000 * (±0.000) 0.000 * (±0.000) 0.000 * (±0.000)SVM-RBF 0.002 (±0.003) 0.001 (±0.001) 0.001 (±0.001)GBM 0.002 (±0.003) 0.001 (±0.001) 0.001 (±0.001)Category& itemLR 0.074 (±0.005) 0.186 (±0.013) 0.106 (±0.007)SVM-RBF 0.134 (±0.006) 0.335 (±0.015) 0.192 (±0.009)GBM 0.051 (±0.005) 0.126 (±0.012) 0.072 (±0.007)Date& timeLR 0.137 (±0.020) 0.069 (±0.010) 0.092 (±0.013)SVM-RBF 0.225 (±0.021) 0.113 (±0.010) 0.150 (±0.014)GBM 0.406 (±0.025) 0.203 (±0.012) 0.271 (±0.017)OthersLR 0.000 * (±0.000) 0.000 * (±0.000) 0.000 * (±0.000)SVM-RBF 0.027 (±0.015) 0.008 (±0.005) 0.012 (±0.007)GBM 0.065 (±0.024) 0.020 (±0.007) 0.030 (±0.011)Note: * unadjusted 𝑝 < 0.01, for 𝐻0: 𝜇 > 0, 𝐻𝐴: 𝜇 ≤ 0Source: AuthorsWe test the difference in mean point estimates of the information retrieval measures for feature importance ranksgenerated by predictive solutions and their randomly permutated counterparts to explicitly compensate for groupsize. The outcomes are displayed in Table 7, consisting of difference estimates and confidence bounds, and furthervalidated with the bootstrap tests. Recency and Frequency surpass their referential sets in all aspects. Date & timealso performs well when coupled with GBM. The remaining feature groups seem inferior.Table 7. Difference in mean point estimates between the observed and referential feature set importance.Set Algorithm ΔPRE (95% CI) ΔREC (95% CI) ΔF1 (95% CI)RecencyLR 0.651* (±0.017) 0.325* (±0.009) 0.434* (±0.011)SVM-RBF 0.438* (±0.021) 0.219* (±0.011) 0.292* (±0.014)GBM 0.592* (±0.022) 0.296* (±0.011) 0.395* (±0.014)FrequencyLR 0.312* (±0.016) 0.187* (±0.010) 0.234* (±0.012)SVM-RBF 0.150 * (±0.021) 0.090 * (±0.012) 0.112* (±0.015)GBM 0.201* (±0.017) 0.121* (±0.010) 0.151* (±0.012)MonetaryLR −0.212 (±0.005) −0.064 (±0.001) −0.098 (±0.002)SVM-RBF −0.211 (±0.006) −0.063 (±0.002) −0.097 (±0.003)GBM −0.211 (±0.005) −0.063 (±0.002) −0.097 (±0.003)Category& itemLR −0.139 (±0.005) −0.347 (±0.013) −0.198 (±0.008)SVM-RBF −0.079 (±0.006) −0.197 (±0.015) −0.113 (±0.009)GBM −0.162 (±0.005) −0.406 (±0.011) −0.232 (±0.007)Date& timeLR −0.075 (±0.020) −0.037 (±0.010) −0.050 (±0.013)SVM-RBF 0.013 (±0.021) 0.006 (±0.010) 0.009 (±0.014)GBM 0.194* (±0.024) 0.097* (±0.012) 0.129* (±0.016)OthersLR −0.213 (±0.005) −0.064 (±0.001) −0.098 (±0.002)SVM-RBF −0.186 (±0.017) −0.056 (±0.005) −0.086 (±0.008)GBM −0.148 (±0.024) −0.044 (±0.007) −0.068 (±0.011)Note: * unadjusted 𝑝 < 0.01, for 𝐻0: ∆𝜇 ≤ 0, 𝐻𝐴: ∆𝜇 > 0Source: Authors
Page 10
10 SciPap 30(1)DiscussionIndividual importance revealed the ses_rec, ses_rec_avg, and ses_n variables as the most influential. Gordini andVeglio (2017) and Li and Li (2019) rely on their transactional counterparts. Rachid et al. (2018) depend on featuresdescribing users' behavior within a purchasing process. The disparities are driven by churn perception andbusiness context, e.g., the reported studies focus on transactional churn in explained and explanatory variables.Other culprits may include learners, evaluation procedures, or the use of data partitions.Set perspective recognized Recency and Frequency as the most important regarding information retrievalmeasures and matching differences across LR, SVM-RBF, and GBM. Berger and Kompan (2019) also presentnotable classification performance gains when expanding the base user model with analogous attributes. TheCategory & item and Date & time groups displayed moderate relevance when coupled with SVM-RBF or GBM;nevertheless, they were underwhelming when adjusted for the number of elements. This problem might bealleviated with variable preselection, dense encodings, and additional feature engineering. Correspondingly,Gordini and Veglio (2017) associate transactional preferences with churn events. Monetary & Other characteristicsseemed inferior. There does not appear to be general agreement on the importance of the former set; e.g., Bergerand Kompan (2019) present contradictory results while introducing the Monetary features.Our findings support some of the standard dimensions of customer analytics and expose the possible value ofpreference and date-time behavioural patterns. Further local comprehension of suitable modeling pipelines mayassist retention management professionals in leveraging imprinted inclinations to formulate personalized valuepropositions and campaign schedules. The global interpretation might expose problematic product segments orunsatisfactory user experience. Other directions for future research may involve a broader spectrum of businesscontexts. Other feature groups can inform the user model, such as geospatial or perceptual characteristics. Newinsights may be supported by diverse classification learners, hyperparameter optimization, and looser pipelineselection criteria. In addition, future research endeavours might divert from predictive to causal modeling andexamine the structure of the underlying decision-making process. Schiffman et al. (2012) suggest enlighteningsuch aspiration with the external influences (company and its environment), the process itself (decision stages,experience, psychological aspects), and its outputs (purchase or usage, post-evaluation); the redirection mayfurther inform consumer marketing and behavior theory.The model-agnostic set evaluation procedure balances aspects of effects on predictive performance and overallquality. It also accounts for interactions amongst the explained variables. Unfortunately, the impact on performanceis assessed indirectly. Other shortcomings include the arbitrary size of the most influential group or the omissionof its inner rankings. Thus, we suggest further expansion with comprehensive sensitivity analysis and rank-awaremetrics.ConclusionsOver the last twenty years, technological innovations and the transition towards user-centric thinking have fueledthe rise of the retail e-commerce sector. It has become imperative to maintain a healthy user/customer basethrough retention management. Churn prediction informs both short- and long-term retention pursuits and reliesupon dependent and independent attributes. However, the research literature does not demonstrate agreementon such a user/customer churn model. Thus, we proposed a user churn model suitable for e-commerce retail andinvestigated its properties using auxiliary evaluation. We shaped the model around interactions with the websiteand covered various aspects of user behavior. The indirect assessment of permutation importance was carriedout on unseen data employing the best performing solutions, namely, LR, SVM-RBF, and GBM.Individual importance acknowledged the period from the last session, the average period between the sessions,and the number of sessions as the essential features. Similarly, the set perspective recognized the importance ofrecency and frequency characteristics concerning information retrieval metrics and matching differences acrossthe relevant modeling pipelines. Furthermore, SVM-RBF and GBM learners supported new feature groups suchas Category & Item or Date & time. The remaining sets manifested poor associations with the classificationperformance. The findings might also inform retention management endeavours, e.g., personalized campaigns oruser experience enhancements. Other contributions include the original set evaluation procedure and the opendataset.Future research may address the limitations of our work with broader investigation across multiple businesscontexts. The user model can be informed by other feature groups. In addition, we suggest interpreting theunderlying associations using partial-dependence plots, Shapley values, or surrogate models. New insights mayalso be supported by employing diverse classification learners, hyperparameter optimization, and looser pipelineselection criteria. In addition, future scientific efforts may turn away from predictive to causal modeling andinvestigate the structure of the underlying decision-making process; the shift may inform consumer marketing andbehavior theory.The set evaluation procedure might be notably extended with information retrieval metrics directly linked to the
Page 11
11 SciPap 30(1)solutions' predictive performance and inner importance ranks. Furthermore, a thorough sensitivity analysis mightbe advisable.Supplementary Materials:Code: https://github.com/fridrichmrtn/user-churn-model-ecommerce-retailDataset: https://www.kaggle.com/fridrichmrtn/user-churn-datasetReferencesAbbasi, A., Lau, R. Y. K., & Brown, D. E. (2015). Predicting behavior. IEEE Intelligent Systems, 30(3), 35-43.https://doi.org/10.1109/MIS.2015.19Aggarwal, C. C. (2014). Data classification: algorithms and applications. Taylor & Francis.Almuqren, L., Alrayes, F. S., & Cristea, A. I. (2021). An Empirical Study on Customer Churn Behaviours Prediction Using ArabicTwitter Mining Approach. Future Internet, 13(7). https://doi.org/10.3390/fi13070175Ascarza, E., Neslin, S., Netzer, O., Anderson, Z., Fader, P., Gupta, S., Hardie, B., Lemmens, A., Libai, B., Neal, D., Provost, F.,& Schrift, R. (2018). In Pursuit of Enhanced Customer Retention Management: Review, Key Issues, and FutureDirections. Customer Needs and Solutions, 5(1), 65-81. https://doi.org/10.1007/s40547-017-0080-0Berger, P., & Kompan, M. (2019). User Modeling for Churn Prediction in E-Commerce. IEEE Intelligent Systems, 34(2), 44-52.https://doi.org/10.1109/MIS.2019.2895788Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32. https://doi.org/10.1023/A:1010933404324Brusilovsky, P. (1996). Methods and techniques of adaptive hypermedia. User Modeling and User-Adapted Interaction, 6(2-3),87-129. https://doi.org/10.1007/BF00143964Buckinx, W., & Van den Poel, D. (2005). Customer base analysis: partial defection of behaviourally loyal clients in a non-contractual FMCG retail setting. European Journal of Operational Research, 164(1), 252-268.https://doi.org/10.1016/j.ejor.2003.12.010Clemente-Císcar, M., San Matías, S., & Giner-Bosch, V. (2014). A methodology based on profitability criteria for defining thepartial defection of customers in non-contractual settings. European Journal of Operational Research, 239(1), 276-285.https://doi.org/10.1016/j.ejor.2014.04.029Fader, P. S., Hardie, B. G. S., & Lee, K. L. (2005). "Counting your customers" the easy way: an alternative to the Pareto/NBDModel. Marketing Science, 24(2), 275. https://doi.org/10.1287/mksc.1040.0098Glady, N., Baesens, B., & Croux, C. (2009). Modeling churn using customer lifetime value. European Journal of OperationalResearch, 197(1), 402-411. https://doi.org/10.1016/j.ejor.2008.06.027Gordini, N., & Veglio, V. (2017). Customers churn prediction and marketing retention strategies. An application of support vectormachines based on the AUC parameter-selection technique in B2B e-commerce industry. Industrial MarketingManagement, 62, 100-107. https://doi.org/10.1016/j.indmarman.2016.08.003Gronwald, K. D. (2017). Integrated Business Information Systems: A Holistic View of the Linked Business Process Chain ERP-SCM-CRM-BI-Big Data: A Holistic View of the Linked Business Process Chain ERP-SCM-CRM-BI-Big Data. SpringerBerlin Heidelberg. https://books.google.cz/books?id=mSYmDwAAQBAJHand, D. J. (2009). Measuring classifier performance: a coherent alternative to the Area under the ROC curve. MachineLearning, 77(1), 103-123. https://doi.org/10.1007/s10994-009-5119-5Hughes, A. M. (2012). Strategic database marketing (4th ed). McGraw-Hill.Chou, Y. C., & Chuang, H. H. -C. (2018). A predictive investigation of first-time customer retention in online reservation services.Service Business, 12(4), 685-699. https://doi.org/10.1007/s11628-018-0371-zKim, K., & Lee, J. (2012). Sequential manifold learning for efficient churn prediction. Expert Systems with Applications, 39(18),13328-13337. https://doi.org/10.1016/j.eswa.2012.05.069Li, X., & Li, Z. (2019). A Hybrid Prediction Model for E-Commerce Customer Churn Based on Logistic Regression and ExtremeGradient Boosting Algorithm. Ingénierie des systèmes d information, 24(5), 525-530.https://doi.org/10.18280/isi.240510Llave Montiel, M. A., & López, F. (2020). Spatial models for online retail churn: Evidence from an online grocery delivery servicein Madrid. Papers in Regional Science, 99(6), 1643-1665. https://doi.org/10.1111/pirs.12552MacKenzie, I., Meyer, C., & Noble, S. (2013). How retailers can keep up with consumers. McKInsey & Company Insights.https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumersMezghani, M., Zayani, C. A., Amous, I., & Gargouri, F. (2012). A user profile modelling using social annotations. In Proceedingsof the 21st international conference companion on World Wide Web - WWW '12 Companion (p. 969-). ACM Press.https://doi.org/10.1145/2187980.2188230Morgan, B. (2018). How Amazon Has Reorganized Around Artificial Intelligence and Machine Learning. Forbes.https://www.forbes.com/sites/blakemorgan/2018/07/16/how-amazon-has-re-organized-around-artificial-intelligence-and-machine-learning/Mozer, M. C., Wolniewicz, R., Grimes, D. B., Johnson, E., & Kaushansky, H. (2000). Predicting subscriber dissatisfaction andimproving retention in the wireless telecommunications industry. IEEE Transactions on Neural Networks, 11(3), 690-696. https://doi.org/10.1109/72.846740Netzer, O., Lattin, J. M., & Srinivasan, V. (2008). A Hidden Markov Model of Customer Relationship Dynamics. MarketingScience, 27(2), 185-204. https://doi.org/10.1287/mksc.1070.0294Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, Weiss, R., Dubourg,V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, É. (2011). Scikit-learn: MachineLearning in Python. Journal of Machine Learning Research, 12(85), 2825-2830.
Page 12
12 SciPap 30(1)https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.htmlPerisic, A., & Pahor, M. RFM-LIR feature framework for churn prediction in the mobile games market. IEEE Transactions onGames, 1-1. https://doi.org/10.1109/TG.2021.3067114Rachid, A. D., Abdellah, A., Belaid, B., & Rachid, L. (2018). Clustering Prediction Techniques in Defining and PredictingCustomers Defection: The Case of E-Commerce Context. International Journal of Electrical and Computer Engineering,8(4), 2367-2383. https://doi.org/10.11591/ijece.v8i4.pp2367-2383Rothmeier, K., Pflanzl, N., Hullmann, J. A., & Preuss, M. (2021). Prediction of Player Churn and Disengagement Based on UserActivity Data of a Freemium Online Strategy Game. IEEE Transactions on Games, 13(1), 78-88.https://doi.org/10.1109/TG.2020.2992282Schiffman, L. G., Kanuk, L. L., & Hansen, H. (2012). Consumer Behaviour: A European Outlook (2nd). Pearson FinancialTimes/Prentice Hall.Tamaddoni Jahromi, A., Stakhovych, S., & Ewing, M. (2014). Managing B2B customer churn, retention and profitability. IndustrialMarketing Management, 43(7), 1258-1268. https://doi.org/10.1016/j.indmarman.2014.06.016Terdiman, D. (2018). How AI is helping Amazon become a trillion-dollar company. Fast company.https://www.fastcompany.com/90246028/how-ai-is-helping-amazon-become-a-trillion-dollar-companyWilliams, C., & Seeger, M. (2001). Using the nyström method to speed up kernel machines. In Advances in Neural InformationProcessing Systems 13 (pp. 682-688). MIT Press. https://infoscience.epfl.ch/record/161322?ln=enYu, X., Guo, S., Guo, J., & Huang, X. (2011). An extended support vector machine forecasting framework for customer churn ine-commerce. Expert Systems with Applications, 38(3), 1425-1430. https://doi.org/10.1016/j.eswa.2010.07.049Retailrocket recommender system dataset: Ecommerce data: web events, item properties (with texts), category tree. (2017).Kaggle. Retrieved June 7, 2021, from https://www.kaggle.com/retailrocket/ecommerce-dataset/metadata 

(PDF) User Churn Model in E-Commerce Retail. Available from: https://www.researchgate.net/publication/359739936_User_churn_model_in_e-commerce_retail [accessed Sep 27 2025].