{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KEYrzG2vB8Ip"
      },
      "outputs": [],
      "source": [
        "# Important!!!\n",
        "#\n",
        "# <---- Set your 'OPENAI_API_KEY' as a secret over there with the \"key\" icon\n",
        "#\n",
        "# <---- You will also likely want to use the \"folder\" icon to add some files\n",
        "#       for the agent to look at\n",
        "#\n",
        "import os\n",
        "with open(\"../../openai_token\", \"r\") as file:\n",
        "    openai_token = file.read().strip()\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(\"../../\")  # Change to the root directory of the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mwe2eeOQB0cC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import traceback\n",
        "from litellm import completion\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Callable, Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class Prompt:\n",
        "    messages: List[Dict] = field(default_factory=list)\n",
        "    tools: List[Dict] = field(default_factory=list)\n",
        "    metadata: dict = field(default_factory=dict)  # Fixing mutable default issue\n",
        "\n",
        "\n",
        "def generate_response(prompt: Prompt) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "\n",
        "    messages = prompt.messages\n",
        "    tools = prompt.tools\n",
        "\n",
        "    result = None\n",
        "\n",
        "    if not tools:\n",
        "        response = completion(\n",
        "            model=\"openai/gpt-4o\",\n",
        "            messages=messages,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "    else:\n",
        "        response = completion(\n",
        "            model=\"openai/gpt-4o\",\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "\n",
        "        if response.choices[0].message.tool_calls:\n",
        "            tool = response.choices[0].message.tool_calls[0]\n",
        "            result = {\n",
        "                \"tool\": tool.function.name,\n",
        "                \"args\": json.loads(tool.function.arguments),\n",
        "            }\n",
        "            result = json.dumps(result)\n",
        "        else:\n",
        "            result = response.choices[0].message.content\n",
        "\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "class Action:\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 function: Callable,\n",
        "                 description: str,\n",
        "                 parameters: Dict,\n",
        "                 terminal: bool = False):\n",
        "        self.name = name\n",
        "        self.function = function\n",
        "        self.description = description\n",
        "        self.terminal = terminal\n",
        "        self.parameters = parameters\n",
        "\n",
        "    def execute(self, **args) -> Any:\n",
        "        \"\"\"Execute the action's function\"\"\"\n",
        "        return self.function(**args)\n",
        "\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self.actions = {}\n",
        "\n",
        "    def register(self, action: Action):\n",
        "        self.actions[action.name] = action\n",
        "\n",
        "    def get_action(self, name: str) -> [Action, None]:\n",
        "        return self.actions.get(name, None)\n",
        "\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        \"\"\"Get all registered actions\"\"\"\n",
        "        return list(self.actions.values())\n",
        "\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items = []  # Basic conversation histor\n",
        "\n",
        "    def add_memory(self, memory: dict):\n",
        "        \"\"\"Add memory to working memory\"\"\"\n",
        "        self.items.append(memory)\n",
        "\n",
        "    def get_memories(self, limit: int = None) -> List[Dict]:\n",
        "        \"\"\"Get formatted conversation history for prompt\"\"\"\n",
        "        return self.items[:limit]\n",
        "\n",
        "    def copy_without_system_memories(self):\n",
        "        \"\"\"Return a copy of the memory without system memories\"\"\"\n",
        "        filtered_items = [m for m in self.items if m[\"type\"] != \"system\"]\n",
        "        memory = Memory()\n",
        "        memory.items = filtered_items\n",
        "        return memory\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: dict) -> dict:\n",
        "        \"\"\"Execute an action and return the result.\"\"\"\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return self.format_result(result)\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool_executed\": False,\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "\n",
        "    def format_result(self, result: Any) -> dict:\n",
        "        \"\"\"Format the result with metadata.\"\"\"\n",
        "        return {\n",
        "            \"tool_executed\": True,\n",
        "            \"result\": result,\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "        }\n",
        "\n",
        "\n",
        "class AgentLanguage:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "\n",
        "\n",
        "class AgentFunctionCallingActionLanguage(AgentLanguage):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def format_goals(self, goals: List[Goal]) -> List:\n",
        "        # Map all goals to a single string that concatenates their description\n",
        "        # and combine into a single message of type system\n",
        "        sep = \"\\n-------------------\\n\"\n",
        "        goal_instructions = \"\\n\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": goal_instructions}\n",
        "        ]\n",
        "\n",
        "    def format_memory(self, memory: Memory) -> List:\n",
        "        \"\"\"Generate response from language model\"\"\"\n",
        "        # Map all environment results to a role:user messages\n",
        "        # Map all assistant messages to a role:assistant messages\n",
        "        # Map all user messages to a role:user messages\n",
        "        items = memory.get_memories()\n",
        "        mapped_items = []\n",
        "        for item in items:\n",
        "\n",
        "            content = item.get(\"content\", None)\n",
        "            if not content:\n",
        "                content = json.dumps(item, indent=4)\n",
        "\n",
        "            if item[\"type\"] == \"assistant\":\n",
        "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "            elif item[\"type\"] == \"environment\":\n",
        "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "            else:\n",
        "                mapped_items.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "        return mapped_items\n",
        "\n",
        "    def format_actions(self, actions: List[Action]) -> [List,List]:\n",
        "        \"\"\"Generate response from language model\"\"\"\n",
        "\n",
        "        tools = [\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": action.name,\n",
        "                    # Include up to 1024 characters of the description\n",
        "                    \"description\": action.description[:1024],\n",
        "                    \"parameters\": action.parameters,\n",
        "                },\n",
        "            } for action in actions\n",
        "        ]\n",
        "\n",
        "        return tools\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "\n",
        "        prompt = []\n",
        "        prompt += self.format_goals(goals)\n",
        "        prompt += self.format_memory(memory)\n",
        "\n",
        "        tools = self.format_actions(actions)\n",
        "\n",
        "        return Prompt(messages=prompt, tools=tools)\n",
        "\n",
        "    def adapt_prompt_after_parsing_error(self,\n",
        "                                         prompt: Prompt,\n",
        "                                         response: str,\n",
        "                                         traceback: str,\n",
        "                                         error: Any,\n",
        "                                         retries_left: int) -> Prompt:\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        \"\"\"Parse LLM response into structured format by extracting the ```json block\"\"\"\n",
        "\n",
        "        try:\n",
        "            return json.loads(response)\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool\": \"terminate\",\n",
        "                \"args\": {\"message\":response}\n",
        "            }\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 goals: List[Goal],\n",
        "                 agent_language: AgentLanguage,\n",
        "                 action_registry: ActionRegistry,\n",
        "                 generate_response: Callable[[Prompt], str],\n",
        "                 environment: Environment):\n",
        "        \"\"\"\n",
        "        Initialize an agent with its core GAME components\n",
        "        \"\"\"\n",
        "        self.goals = goals\n",
        "        self.generate_response = generate_response\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
        "        \"\"\"Build prompt with memory context\"\"\"\n",
        "        return self.agent_language.construct_prompt(\n",
        "            actions=actions.get_actions(),\n",
        "            environment=self.environment,\n",
        "            goals=goals,\n",
        "            memory=memory\n",
        "        )\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation[\"tool\"])\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response: str) -> bool:\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return action_def.terminal\n",
        "\n",
        "    def set_current_task(self, memory: Memory, task: str):\n",
        "        memory.add_memory({\"type\": \"user\", \"content\": task})\n",
        "\n",
        "    def update_memory(self, memory: Memory, response: str, result: dict):\n",
        "        \"\"\"\n",
        "        Update memory with the agent's decision and the environment's response.\n",
        "        \"\"\"\n",
        "        new_memories = [\n",
        "            {\"type\": \"assistant\", \"content\": response},\n",
        "            {\"type\": \"environment\", \"content\": json.dumps(result)}\n",
        "        ]\n",
        "        for m in new_memories:\n",
        "            memory.add_memory(m)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
        "        response = self.generate_response(full_prompt)\n",
        "        return response\n",
        "\n",
        "    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\n",
        "        \"\"\"\n",
        "        Execute the GAME loop for this agent with a maximum iteration limit.\n",
        "        \"\"\"\n",
        "        memory = memory or Memory()\n",
        "        self.set_current_task(memory, user_input)\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            # Construct a prompt that includes the Goals, Actions, and the current Memory\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "\n",
        "            print(\"Agent thinking...\")\n",
        "            # Generate a response from the agent\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            print(f\"Agent Decision: {response}\")\n",
        "\n",
        "            # Determine which action the agent wants to execute\n",
        "            action, invocation = self.get_action(response)\n",
        "\n",
        "            # Execute the action in the environment\n",
        "            result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "            print(f\"Action Result: {result}\")\n",
        "\n",
        "            # Update the agent's memory with information about what happened\n",
        "            self.update_memory(memory, response, result)\n",
        "\n",
        "            # Check if the agent has decided to terminate\n",
        "            if self.should_terminate(response):\n",
        "                break\n",
        "\n",
        "        return memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PC3ncxezoJC",
        "outputId": "564b75ed-bcc3-4f59-c51f-ed4e39d7bf90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent thinking...\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['random_hyperparam_optimization.py'], 'timestamp': '2025-09-27T12:00:01+0200'}\n",
            "Agent thinking...\n",
            "Agent Decision: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"random_hyperparam_optimization.py\"}}\n",
            "Action Result: {'tool_executed': True, 'result': 'from sklearn.model_selection import RandomizedSearchCV\\n\\n\\ndef random_hyperparam_optimization(model, param_distributions, X_train, y_train,\\n                                   cv=5, scoring=\\'accuracy\\', n_iter=10, random_state=None, n_jobs=-1):\\n    \"\"\"\\n    Perform random hyperparameter optimization for a given model using RandomizedSearchCV.\\n\\n    Parameters:\\n    ----------\\n    model : estimator object\\n        The machine learning model instance (e.g., RandomForestClassifier) from sklearn.\\n    \\n    param_distributions : dict\\n        Dictionary with parameter names (str) as keys and lists of parameter settings to try as values.\\n        Each key-value pair defines one parameter and its possible values.\\n    \\n    X_train : array-like or sparse matrix, shape (n_samples, n_features)\\n        The training input samples.\\n    \\n    y_train : array-like, shape (n_samples,)\\n        The target values (class labels) as integers or strings.\\n    \\n    cv : int, default=5\\n        Determines the cross-validation splitting strategy. Specify the number of folds.\\n    \\n    scoring : str or callable, default=\\'accuracy\\'\\n        A string or a scorer callable object/function with signature `scorer(estimator, X, y)`.\\n    \\n    n_iter : int, default=10\\n        Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\\n    \\n    random_state : int, RandomState instance or None, default=None\\n        Controls the randomness of the search. Pass an int for reproducible output.\\n    \\n    n_jobs : int, default=-1\\n        Number of jobs to run in parallel. -1 means using all processors.\\n\\n    Returns:\\n    -------\\n    best_model : fitted estimator\\n        The model instance with the best found hyperparameters.\\n\\n    best_params : dict\\n        Parameter setting that gave the best results on the hold out data.\\n\\n    best_score : float\\n        Mean cross-validated score of the best_estimator.\\n\\n    Example Usage:\\n    -------------\\n    >>> from sklearn.ensemble import RandomForestClassifier\\n    >>> model = RandomForestClassifier()\\n    >>> param_distributions = {\\n    ...     \\'n_estimators\\': [10, 50, 100],\\n    ...     \\'max_depth\\': [5, 10, None]\\n    ... }\\n    >>> X_train, y_train = load_training_data()  # Replace with your data loading function\\n    >>> best_model, best_params, best_score = random_hyperparam_optimization(\\n    ...     model, param_distributions, X_train, y_train, cv=3, n_iter=5)\\n    >>> print(best_params)\\n    {\\'n_estimators\\': 50, \\'max_depth\\': None}\\n    >>> print(best_score)\\n    0.85\\n\\n    Edge Cases:\\n    ----------\\n    - If `param_distributions` is empty, RandomSearchCV will raise a ValueError.\\n    - Ensure `X_train` and `y_train` have matching dimensions and contain valid data.\\n    - If `n_iter` is larger than the possible number of parameter combinations, it might raise a warning but will still function.\\n    - Passing incorrect type for model or param_distributions will raise a TypeError.\\n    \"\"\"\\n\\n    random_search = RandomizedSearchCV(\\n        estimator=model,\\n        param_distributions=param_distributions,\\n        n_iter=n_iter,\\n        cv=cv,\\n        scoring=scoring,\\n        random_state=random_state,\\n        n_jobs=n_jobs\\n    )\\n\\n    random_search.fit(X_train, y_train)\\n\\n    best_model = random_search.best_estimator_\\n    best_params = random_search.best_params_\\n    best_score = random_search.best_score_\\n\\n    return best_model, best_params, best_score\\n\\nimport unittest\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.exceptions import NotFittedError\\n\\n\\nclass TestRandomHyperparamOptimization(unittest.TestCase):\\n    def setUp(self):\\n        # Create a toy dataset for testing\\n        self.X_train, self.y_train = make_classification(n_samples=100, n_features=10, random_state=42)\\n\\n    def test_typical_usage(self):\\n        model = RandomForestClassifier()\\n        param_distributions = {\\'n_estimators\\': [10, 50], \\'max_depth\\': [5, None]}\\n        best_model, best_params, best_score = random_hyperparam_optimization(\\n            model, param_distributions, self.X_train, self.y_train,\\n            cv=3, scoring=\\'accuracy\\', n_iter=2, random_state=42\\n        )\\n        self.assertIn(best_params[\\'n_estimators\\'], [10, 50])\\n        self.assertIn(best_params[\\'max_depth\\'], [5, None])\\n        self.assertIsInstance(best_score, float)\\n        self.assertTrue(best_score >= 0)\\n\\n    def test_empty_param_distributions(self):\\n        model = RandomForestClassifier()\\n        with self.assertRaises(ValueError):\\n            random_hyperparam_optimization(model, {}, self.X_train, self.y_train)\\n\\n    def test_invalid_n_iter(self):\\n        model = RandomForestClassifier()\\n        param_distributions = {\\'n_estimators\\': [10]}\\n        with self.assertRaises(ValueError):\\n            random_hyperparam_optimization(\\n                model, param_distributions, self.X_train, self.y_train, n_iter=0\\n            )\\n\\n    def test_invalid_model(self):\\n        param_distributions = {\\'n_estimators\\': [10]}\\n        with self.assertRaises(TypeError):\\n            random_hyperparam_optimization(\\n                None, param_distributions, self.X_train, self.y_train\\n            )\\n\\n    def test_invalid_data(self):\\n        model = RandomForestClassifier()\\n        param_distributions = {\\'n_estimators\\': [10]}\\n        with self.assertRaises(ValueError):\\n            random_hyperparam_optimization(model, param_distributions, None, None)\\n\\n    def test_invalid_param_distributions(self):\\n        model = RandomForestClassifier()\\n        with self.assertRaises(ValueError):\\n            random_hyperparam_optimization(model, {\\'invalid_param\\': None}, self.X_train, self.y_train)\\n\\n\\nif __name__ == \\'__main__\\':\\n    unittest.main()\\n', 'timestamp': '2025-09-27T12:00:03+0200'}\n",
            "Agent thinking...\n",
            "Agent Decision: {\"tool\": \"terminate\", \"args\": {\"message\": \"# Random Hyperparameter Optimization\\n\\nThis project provides a Python implementation for random hyperparameter optimization using `RandomizedSearchCV` from the scikit-learn library. The main functionality is encapsulated in the function `random_hyperparam_optimization`, which allows for efficient tuning of model parameters using a randomized search strategy.\\n\\n## File Overview\\n\\n- **random_hyperparam_optimization.py**: This is the main file containing the implementation of the hyperparameter optimization function and test cases for validating its functionality.\\n\\n## Features\\n\\n- **Flexible Model Input**: It supports any machine learning model compatible with scikit-learn's `RandomizedSearchCV`.\\n- **Customizable Parameters**: Users can specify the parameter distributions to explore, the number of iterations, cross-validation strategy, and scoring method.\\n- **Parallel Processing**: It enables parallel computation to utilize multiple processors, thus speeding up the optimization process.\\n\\n## Usage\\n\\n1. **Setup the Model and Parameters**\\n    ```python\\n    from sklearn.ensemble import RandomForestClassifier\\n    model = RandomForestClassifier()\\n    param_distributions = {\\n        'n_estimators': [10, 50, 100],\\n        'max_depth': [5, 10, None]\\n    }\\n    ````\\n\\n2. **Prepare Training Data**\\n    ```python\\n    X_train, y_train = load_training_data()  # Replace with your data loading function\\n    ```\\n\\n3. **Optimize Hyperparameters**\\n    ```python\\n    best_model, best_params, best_score = random_hyperparam_optimization(\\n        model, param_distributions, X_train, y_train, cv=3, n_iter=5\\n    )\\n    print(best_params)\\n    print(best_score)\\n    ```\\n\\n## Testing\\n\\nUnit tests are provided in the file using Python's `unittest` framework. These tests validate various aspects of the function, from typical usage to handling edge cases.\\n\\n### Running Tests\\n\\nExecute the test suite using:\\n```bash\\npython -m unittest random_hyperparam_optimization.py\\n```\\n\\nEnsure to set up a proper Python environment with necessary dependencies before running tests.\\n\\n## Dependencies\\n\\n- scikit-learn\\n- numpy\\n- scipy\\n\\nEnsure these packages are installed in your Python environment to use the provided function.\\n\\n## Contact\\n\\nFor questions or contributions, please reach out to the project maintainers.\"}}\n",
            "Action Result: {'tool_executed': True, 'result': \"# Random Hyperparameter Optimization\\n\\nThis project provides a Python implementation for random hyperparameter optimization using `RandomizedSearchCV` from the scikit-learn library. The main functionality is encapsulated in the function `random_hyperparam_optimization`, which allows for efficient tuning of model parameters using a randomized search strategy.\\n\\n## File Overview\\n\\n- **random_hyperparam_optimization.py**: This is the main file containing the implementation of the hyperparameter optimization function and test cases for validating its functionality.\\n\\n## Features\\n\\n- **Flexible Model Input**: It supports any machine learning model compatible with scikit-learn's `RandomizedSearchCV`.\\n- **Customizable Parameters**: Users can specify the parameter distributions to explore, the number of iterations, cross-validation strategy, and scoring method.\\n- **Parallel Processing**: It enables parallel computation to utilize multiple processors, thus speeding up the optimization process.\\n\\n## Usage\\n\\n1. **Setup the Model and Parameters**\\n    ```python\\n    from sklearn.ensemble import RandomForestClassifier\\n    model = RandomForestClassifier()\\n    param_distributions = {\\n        'n_estimators': [10, 50, 100],\\n        'max_depth': [5, 10, None]\\n    }\\n    ````\\n\\n2. **Prepare Training Data**\\n    ```python\\n    X_train, y_train = load_training_data()  # Replace with your data loading function\\n    ```\\n\\n3. **Optimize Hyperparameters**\\n    ```python\\n    best_model, best_params, best_score = random_hyperparam_optimization(\\n        model, param_distributions, X_train, y_train, cv=3, n_iter=5\\n    )\\n    print(best_params)\\n    print(best_score)\\n    ```\\n\\n## Testing\\n\\nUnit tests are provided in the file using Python's `unittest` framework. These tests validate various aspects of the function, from typical usage to handling edge cases.\\n\\n### Running Tests\\n\\nExecute the test suite using:\\n```bash\\npython -m unittest random_hyperparam_optimization.py\\n```\\n\\nEnsure to set up a proper Python environment with necessary dependencies before running tests.\\n\\n## Dependencies\\n\\n- scikit-learn\\n- numpy\\n- scipy\\n\\nEnsure these packages are installed in your Python environment to use the provided function.\\n\\n## Contact\\n\\nFor questions or contributions, please reach out to the project maintainers.\\nTerminating...\", 'timestamp': '2025-09-27T12:00:18+0200'}\n",
            "[{'type': 'user', 'content': 'Write a README for this project.'}, {'type': 'assistant', 'content': '{\"tool\": \"list_project_files\", \"args\": {}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": [\"random_hyperparam_optimization.py\"], \"timestamp\": \"2025-09-27T12:00:01+0200\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"read_project_file\", \"args\": {\"name\": \"random_hyperparam_optimization.py\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"from sklearn.model_selection import RandomizedSearchCV\\\\n\\\\n\\\\ndef random_hyperparam_optimization(model, param_distributions, X_train, y_train,\\\\n                                   cv=5, scoring=\\'accuracy\\', n_iter=10, random_state=None, n_jobs=-1):\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    Perform random hyperparameter optimization for a given model using RandomizedSearchCV.\\\\n\\\\n    Parameters:\\\\n    ----------\\\\n    model : estimator object\\\\n        The machine learning model instance (e.g., RandomForestClassifier) from sklearn.\\\\n    \\\\n    param_distributions : dict\\\\n        Dictionary with parameter names (str) as keys and lists of parameter settings to try as values.\\\\n        Each key-value pair defines one parameter and its possible values.\\\\n    \\\\n    X_train : array-like or sparse matrix, shape (n_samples, n_features)\\\\n        The training input samples.\\\\n    \\\\n    y_train : array-like, shape (n_samples,)\\\\n        The target values (class labels) as integers or strings.\\\\n    \\\\n    cv : int, default=5\\\\n        Determines the cross-validation splitting strategy. Specify the number of folds.\\\\n    \\\\n    scoring : str or callable, default=\\'accuracy\\'\\\\n        A string or a scorer callable object/function with signature `scorer(estimator, X, y)`.\\\\n    \\\\n    n_iter : int, default=10\\\\n        Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\\\\n    \\\\n    random_state : int, RandomState instance or None, default=None\\\\n        Controls the randomness of the search. Pass an int for reproducible output.\\\\n    \\\\n    n_jobs : int, default=-1\\\\n        Number of jobs to run in parallel. -1 means using all processors.\\\\n\\\\n    Returns:\\\\n    -------\\\\n    best_model : fitted estimator\\\\n        The model instance with the best found hyperparameters.\\\\n\\\\n    best_params : dict\\\\n        Parameter setting that gave the best results on the hold out data.\\\\n\\\\n    best_score : float\\\\n        Mean cross-validated score of the best_estimator.\\\\n\\\\n    Example Usage:\\\\n    -------------\\\\n    >>> from sklearn.ensemble import RandomForestClassifier\\\\n    >>> model = RandomForestClassifier()\\\\n    >>> param_distributions = {\\\\n    ...     \\'n_estimators\\': [10, 50, 100],\\\\n    ...     \\'max_depth\\': [5, 10, None]\\\\n    ... }\\\\n    >>> X_train, y_train = load_training_data()  # Replace with your data loading function\\\\n    >>> best_model, best_params, best_score = random_hyperparam_optimization(\\\\n    ...     model, param_distributions, X_train, y_train, cv=3, n_iter=5)\\\\n    >>> print(best_params)\\\\n    {\\'n_estimators\\': 50, \\'max_depth\\': None}\\\\n    >>> print(best_score)\\\\n    0.85\\\\n\\\\n    Edge Cases:\\\\n    ----------\\\\n    - If `param_distributions` is empty, RandomSearchCV will raise a ValueError.\\\\n    - Ensure `X_train` and `y_train` have matching dimensions and contain valid data.\\\\n    - If `n_iter` is larger than the possible number of parameter combinations, it might raise a warning but will still function.\\\\n    - Passing incorrect type for model or param_distributions will raise a TypeError.\\\\n    \\\\\"\\\\\"\\\\\"\\\\n\\\\n    random_search = RandomizedSearchCV(\\\\n        estimator=model,\\\\n        param_distributions=param_distributions,\\\\n        n_iter=n_iter,\\\\n        cv=cv,\\\\n        scoring=scoring,\\\\n        random_state=random_state,\\\\n        n_jobs=n_jobs\\\\n    )\\\\n\\\\n    random_search.fit(X_train, y_train)\\\\n\\\\n    best_model = random_search.best_estimator_\\\\n    best_params = random_search.best_params_\\\\n    best_score = random_search.best_score_\\\\n\\\\n    return best_model, best_params, best_score\\\\n\\\\nimport unittest\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.datasets import make_classification\\\\nfrom sklearn.exceptions import NotFittedError\\\\n\\\\n\\\\nclass TestRandomHyperparamOptimization(unittest.TestCase):\\\\n    def setUp(self):\\\\n        # Create a toy dataset for testing\\\\n        self.X_train, self.y_train = make_classification(n_samples=100, n_features=10, random_state=42)\\\\n\\\\n    def test_typical_usage(self):\\\\n        model = RandomForestClassifier()\\\\n        param_distributions = {\\'n_estimators\\': [10, 50], \\'max_depth\\': [5, None]}\\\\n        best_model, best_params, best_score = random_hyperparam_optimization(\\\\n            model, param_distributions, self.X_train, self.y_train,\\\\n            cv=3, scoring=\\'accuracy\\', n_iter=2, random_state=42\\\\n        )\\\\n        self.assertIn(best_params[\\'n_estimators\\'], [10, 50])\\\\n        self.assertIn(best_params[\\'max_depth\\'], [5, None])\\\\n        self.assertIsInstance(best_score, float)\\\\n        self.assertTrue(best_score >= 0)\\\\n\\\\n    def test_empty_param_distributions(self):\\\\n        model = RandomForestClassifier()\\\\n        with self.assertRaises(ValueError):\\\\n            random_hyperparam_optimization(model, {}, self.X_train, self.y_train)\\\\n\\\\n    def test_invalid_n_iter(self):\\\\n        model = RandomForestClassifier()\\\\n        param_distributions = {\\'n_estimators\\': [10]}\\\\n        with self.assertRaises(ValueError):\\\\n            random_hyperparam_optimization(\\\\n                model, param_distributions, self.X_train, self.y_train, n_iter=0\\\\n            )\\\\n\\\\n    def test_invalid_model(self):\\\\n        param_distributions = {\\'n_estimators\\': [10]}\\\\n        with self.assertRaises(TypeError):\\\\n            random_hyperparam_optimization(\\\\n                None, param_distributions, self.X_train, self.y_train\\\\n            )\\\\n\\\\n    def test_invalid_data(self):\\\\n        model = RandomForestClassifier()\\\\n        param_distributions = {\\'n_estimators\\': [10]}\\\\n        with self.assertRaises(ValueError):\\\\n            random_hyperparam_optimization(model, param_distributions, None, None)\\\\n\\\\n    def test_invalid_param_distributions(self):\\\\n        model = RandomForestClassifier()\\\\n        with self.assertRaises(ValueError):\\\\n            random_hyperparam_optimization(model, {\\'invalid_param\\': None}, self.X_train, self.y_train)\\\\n\\\\n\\\\nif __name__ == \\'__main__\\':\\\\n    unittest.main()\\\\n\", \"timestamp\": \"2025-09-27T12:00:03+0200\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"terminate\", \"args\": {\"message\": \"# Random Hyperparameter Optimization\\\\n\\\\nThis project provides a Python implementation for random hyperparameter optimization using `RandomizedSearchCV` from the scikit-learn library. The main functionality is encapsulated in the function `random_hyperparam_optimization`, which allows for efficient tuning of model parameters using a randomized search strategy.\\\\n\\\\n## File Overview\\\\n\\\\n- **random_hyperparam_optimization.py**: This is the main file containing the implementation of the hyperparameter optimization function and test cases for validating its functionality.\\\\n\\\\n## Features\\\\n\\\\n- **Flexible Model Input**: It supports any machine learning model compatible with scikit-learn\\'s `RandomizedSearchCV`.\\\\n- **Customizable Parameters**: Users can specify the parameter distributions to explore, the number of iterations, cross-validation strategy, and scoring method.\\\\n- **Parallel Processing**: It enables parallel computation to utilize multiple processors, thus speeding up the optimization process.\\\\n\\\\n## Usage\\\\n\\\\n1. **Setup the Model and Parameters**\\\\n    ```python\\\\n    from sklearn.ensemble import RandomForestClassifier\\\\n    model = RandomForestClassifier()\\\\n    param_distributions = {\\\\n        \\'n_estimators\\': [10, 50, 100],\\\\n        \\'max_depth\\': [5, 10, None]\\\\n    }\\\\n    ````\\\\n\\\\n2. **Prepare Training Data**\\\\n    ```python\\\\n    X_train, y_train = load_training_data()  # Replace with your data loading function\\\\n    ```\\\\n\\\\n3. **Optimize Hyperparameters**\\\\n    ```python\\\\n    best_model, best_params, best_score = random_hyperparam_optimization(\\\\n        model, param_distributions, X_train, y_train, cv=3, n_iter=5\\\\n    )\\\\n    print(best_params)\\\\n    print(best_score)\\\\n    ```\\\\n\\\\n## Testing\\\\n\\\\nUnit tests are provided in the file using Python\\'s `unittest` framework. These tests validate various aspects of the function, from typical usage to handling edge cases.\\\\n\\\\n### Running Tests\\\\n\\\\nExecute the test suite using:\\\\n```bash\\\\npython -m unittest random_hyperparam_optimization.py\\\\n```\\\\n\\\\nEnsure to set up a proper Python environment with necessary dependencies before running tests.\\\\n\\\\n## Dependencies\\\\n\\\\n- scikit-learn\\\\n- numpy\\\\n- scipy\\\\n\\\\nEnsure these packages are installed in your Python environment to use the provided function.\\\\n\\\\n## Contact\\\\n\\\\nFor questions or contributions, please reach out to the project maintainers.\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"# Random Hyperparameter Optimization\\\\n\\\\nThis project provides a Python implementation for random hyperparameter optimization using `RandomizedSearchCV` from the scikit-learn library. The main functionality is encapsulated in the function `random_hyperparam_optimization`, which allows for efficient tuning of model parameters using a randomized search strategy.\\\\n\\\\n## File Overview\\\\n\\\\n- **random_hyperparam_optimization.py**: This is the main file containing the implementation of the hyperparameter optimization function and test cases for validating its functionality.\\\\n\\\\n## Features\\\\n\\\\n- **Flexible Model Input**: It supports any machine learning model compatible with scikit-learn\\'s `RandomizedSearchCV`.\\\\n- **Customizable Parameters**: Users can specify the parameter distributions to explore, the number of iterations, cross-validation strategy, and scoring method.\\\\n- **Parallel Processing**: It enables parallel computation to utilize multiple processors, thus speeding up the optimization process.\\\\n\\\\n## Usage\\\\n\\\\n1. **Setup the Model and Parameters**\\\\n    ```python\\\\n    from sklearn.ensemble import RandomForestClassifier\\\\n    model = RandomForestClassifier()\\\\n    param_distributions = {\\\\n        \\'n_estimators\\': [10, 50, 100],\\\\n        \\'max_depth\\': [5, 10, None]\\\\n    }\\\\n    ````\\\\n\\\\n2. **Prepare Training Data**\\\\n    ```python\\\\n    X_train, y_train = load_training_data()  # Replace with your data loading function\\\\n    ```\\\\n\\\\n3. **Optimize Hyperparameters**\\\\n    ```python\\\\n    best_model, best_params, best_score = random_hyperparam_optimization(\\\\n        model, param_distributions, X_train, y_train, cv=3, n_iter=5\\\\n    )\\\\n    print(best_params)\\\\n    print(best_score)\\\\n    ```\\\\n\\\\n## Testing\\\\n\\\\nUnit tests are provided in the file using Python\\'s `unittest` framework. These tests validate various aspects of the function, from typical usage to handling edge cases.\\\\n\\\\n### Running Tests\\\\n\\\\nExecute the test suite using:\\\\n```bash\\\\npython -m unittest random_hyperparam_optimization.py\\\\n```\\\\n\\\\nEnsure to set up a proper Python environment with necessary dependencies before running tests.\\\\n\\\\n## Dependencies\\\\n\\\\n- scikit-learn\\\\n- numpy\\\\n- scipy\\\\n\\\\nEnsure these packages are installed in your Python environment to use the provided function.\\\\n\\\\n## Contact\\\\n\\\\nFor questions or contributions, please reach out to the project maintainers.\\\\nTerminating...\", \"timestamp\": \"2025-09-27T12:00:18+0200\"}'}]\n"
          ]
        }
      ],
      "source": [
        "    # Define the agent's goals\n",
        "    goals = [\n",
        "        Goal(priority=1, name=\"Gather Information\", description=\"Read each file in the project\"),\n",
        "        Goal(priority=1, name=\"Terminate\", description=\"Call the terminate call when you have read all the files \"\n",
        "                                                       \"and provide the content of the README in the terminate message\")\n",
        "    ]\n",
        "\n",
        "    # Define the agent's language\n",
        "    agent_language = AgentFunctionCallingActionLanguage()\n",
        "\n",
        "    def read_project_file(name: str) -> str:\n",
        "        with open(name, \"r\") as f:\n",
        "            return f.read()\n",
        "\n",
        "    def list_project_files() -> List[str]:\n",
        "        return sorted([file for file in os.listdir(\".\") if file.endswith(\".py\")])\n",
        "\n",
        "\n",
        "    # Define the action registry and register some actions\n",
        "    action_registry = ActionRegistry()\n",
        "    action_registry.register(Action(\n",
        "        name=\"list_project_files\",\n",
        "        function=list_project_files,\n",
        "        description=\"Lists all files in the project.\",\n",
        "        parameters={},\n",
        "        terminal=False\n",
        "    ))\n",
        "    action_registry.register(Action(\n",
        "        name=\"read_project_file\",\n",
        "        function=read_project_file,\n",
        "        description=\"Reads a file from the project.\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\"name\"]\n",
        "        },\n",
        "        terminal=False\n",
        "    ))\n",
        "    action_registry.register(Action(\n",
        "        name=\"terminate\",\n",
        "        function=lambda message: f\"{message}\\nTerminating...\",\n",
        "        description=\"Terminates the session and prints the message to the user.\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"message\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": []\n",
        "        },\n",
        "        terminal=True\n",
        "    ))\n",
        "\n",
        "    # Define the environment\n",
        "    environment = Environment()\n",
        "\n",
        "    # Create an agent instance\n",
        "    agent = Agent(goals, agent_language, action_registry, generate_response, environment)\n",
        "\n",
        "    # Run the agent with user input\n",
        "    user_input = \"Write a README for this project.\"\n",
        "    final_memory = agent.run(user_input)\n",
        "\n",
        "    # Print the final memory\n",
        "    print(final_memory.get_memories())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
